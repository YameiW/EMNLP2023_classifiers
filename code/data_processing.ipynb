{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "from sklearn.utils import shuffle\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "from utils import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(760575, 11)\n",
      "The number of unique nouns is 44620.\n"
     ]
    }
   ],
   "source": [
    "# Import cleaned data\n",
    "df = pd.read_csv('../data/cleaned_data.csv',index_col=[0])\n",
    "df = df.drop_duplicates()\n",
    "df = df[df['clf_id'].astype('int')<df['clf_gov2_id'].astype('int')]\n",
    "print(df.shape)\n",
    "unique_num_noun = len(df['clf_gov2_form'].unique())\n",
    "print(f\"The number of unique nouns is {unique_num_noun}.\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into two structures: df1: clf_noun_structure; df2: clf_mod_noun_structure\n",
    "df1 = df[df['clf_id']==df['clf_gov2_id']-1].reset_index()\n",
    "df2 = df[df['clf_id'] != df['clf_gov2_id'] - 1].reset_index()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### frequency comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### conditional entropy comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the freq information\n",
    "with open(\"../data/leipzig.noun.pkl\",'rb') as file:\n",
    "    nounFreq = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe of modified nouns for two scenarios: clf_noun_structure and clf_mod_noun_structure\n",
    "df1_nounFreq = pd.DataFrame(list(df1.clf_gov2_form.unique()),columns=['noun'])\n",
    "df1_nounFreq['freq'] = df1_nounFreq.noun.map(nounFreq)\n",
    "df2_nounFreq = pd.DataFrame(list(df2.clf_gov2_form.unique()),columns=['noun'])\n",
    "df2_nounFreq['freq'] = df2_nounFreq.noun.map(nounFreq)\n",
    "\n",
    "# remove nouns that are less than or equal to 25 in frequency\n",
    "df1_nounFreq = df1_nounFreq[df1_nounFreq['freq']>25]\n",
    "df2_nounFreq = df2_nounFreq[df2_nounFreq['freq']>25]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample nouns based on their frequency bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_nounFreq['log_freq'] = np.log(df1_nounFreq['freq'])\n",
    "df1_nounFreq['bins'] = pd.cut(df1_nounFreq['log_freq'],bins=30)\n",
    "df1_sample = df1_nounFreq.groupby('bins').apply(lambda x: x.sample(frac = 0.01,replace=False, random_state=1)).reset_index(drop=True)\n",
    "\n",
    "df2_nounFreq['log_freq'] = np.log(df2_nounFreq['freq'])\n",
    "df2_nounFreq['bins'] = pd.cut(df2_nounFreq['log_freq'],bins=30)\n",
    "df2_sample = df2_nounFreq.groupby('bins').apply(lambda x: x.sample(frac = 0.01,replace=False, random_state=1)).reset_index(drop=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate noun pairs\n",
    "##### clf_noun structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_ls1 = df1_sample['noun']\n",
    "pair_ls1 = (list(combinations(noun_ls1,2)))\n",
    "\n",
    "pair_df1 = pd.DataFrame(pair_ls1).rename(columns={0:'noun1',1:'noun2'})\n",
    "a_dict1 = {key:val for key,val in zip(df1_sample['noun'],df1_sample['freq'])}\n",
    "\n",
    "pair_df1['noun1_freq'] = pair_df1['noun1'].map(a_dict1)\n",
    "pair_df1['noun2_freq'] = pair_df1['noun2'].map(a_dict1)\n",
    "\n",
    "pair_df1['noun1_log'] = np.log(pair_df1['noun1_freq'])\n",
    "pair_df1['noun2_log'] = np.log(pair_df1['noun2_freq'])\n",
    "pair_df1 = shuffle(pair_df1).reset_index(drop=True)\n",
    "\n",
    "pair_df1 = balancedFreq_n1_n2(pair_df1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### clf_mod_noun structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_ls2 = df2_sample['noun']\n",
    "pair_ls2 = (list(combinations(noun_ls2,2)))\n",
    "\n",
    "pair_df2 = pd.DataFrame(pair_ls2).rename(columns={0:'noun1',1:'noun2'})\n",
    "a_dict2 = {key:val for key,val in zip(df2_sample['noun'],df2_sample['freq'])}\n",
    "\n",
    "pair_df2['noun1_freq'] = pair_df2['noun1'].map(a_dict2)\n",
    "pair_df2['noun2_freq'] = pair_df2['noun2'].map(a_dict2)\n",
    "\n",
    "pair_df2['noun1_log'] = np.log(pair_df2['noun1_freq'])\n",
    "pair_df2['noun2_log'] = np.log(pair_df2['noun2_freq'])\n",
    "pair_df2 = shuffle(pair_df2).reset_index(drop=True)\n",
    "\n",
    "pair_df2 = balancedFreq_n1_n2(pair_df2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### similarities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "/Users/yameiwang/miniconda3/envs/EMNLP2023_classifier/lib/python3.8/site-packages/scipy/spatial/distance.py:622: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "pair_df1 = similarity(pair_df1,noun_ls1)\n",
    "pair_df2 = similarity(pair_df2,noun_ls2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PMI\n",
    "##### clf_noun structure"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### clf_mod_structure"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### class membership"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_df1 = class_mem_calculator(pair_df1,df,noun_ls1)\n",
    "pair_df2 = class_mem_calculator(pair_df2,df,noun_ls2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EMNLP2023_classifier",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
