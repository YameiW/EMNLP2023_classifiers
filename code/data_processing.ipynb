{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "from utils import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(760575, 11)\n",
      "The number of unique nouns is 44620.\n"
     ]
    }
   ],
   "source": [
    "# Import cleaned data\n",
    "df = pd.read_csv('../data/cleaned_data.csv',index_col=[0])\n",
    "df = df.drop_duplicates()\n",
    "df = df[df['clf_id'].astype('int')<df['clf_gov2_id'].astype('int')]\n",
    "print(df.shape)\n",
    "unique_num_noun = len(df['clf_gov2_form'].unique())\n",
    "print(f\"The number of unique nouns is {unique_num_noun}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into two structures: df1: clf_noun_structure; df2: clf_mod_noun_structure\n",
    "df1 = df[df['clf_id']==df['clf_gov2_id']-1].reset_index()\n",
    "df2 = df[df['clf_id'] != df['clf_gov2_id'] - 1].reset_index()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the freq information\n",
    "with open(\"../data/leipzig.noun.pkl\",'rb') as file:\n",
    "    nounFreq = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe of modified nouns for two scenarios: clf_noun_structure and clf_mod_noun_structure\n",
    "df1_nounFreq = pd.DataFrame(list(df1.clf_gov2_form.unique()),columns=['noun'])\n",
    "df1_nounFreq['freq'] = df1_nounFreq.noun.map(nounFreq)\n",
    "df2_nounFreq = pd.DataFrame(list(df2.clf_gov2_form.unique()),columns=['noun'])\n",
    "df2_nounFreq['freq'] = df2_nounFreq.noun.map(nounFreq)\n",
    "\n",
    "# remove nouns that are less than or equal to 25 in frequency\n",
    "df1_nounFreq = df1_nounFreq[df1_nounFreq['freq']>25]\n",
    "df2_nounFreq = df2_nounFreq[df2_nounFreq['freq']>25]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample nouns based on their frequency bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_nounFreq['log_freq'] = np.log(df1_nounFreq['freq'])\n",
    "df1_nounFreq['bins'] = pd.cut(df1_nounFreq['log_freq'],bins=30)\n",
    "df1_sample = df1_nounFreq.groupby('bins').apply(lambda x: x.sample(frac = 0.01,replace=False, random_state=1)).reset_index(drop=True)\n",
    "\n",
    "df2_nounFreq['log_freq'] = np.log(df2_nounFreq['freq'])\n",
    "df2_nounFreq['bins'] = pd.cut(df2_nounFreq['log_freq'],bins=30)\n",
    "df2_sample = df2_nounFreq.groupby('bins').apply(lambda x: x.sample(frac = 0.01,replace=False, random_state=1)).reset_index(drop=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate noun pairs\n",
    "##### clf_noun structure\n",
    "revise this and make it simplier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nounPair_df(noun_ls, a_dict):\n",
    "    '''\n",
    "    This function generates noun pairs based on sampled nouns(noun_ls), and the relative greaterness of n1 and n2 are balanced.\n",
    "    a_dict is a dictionary of nouns and their frequencies.\n",
    "    '''\n",
    "    pair_ls = (list(combinations(noun_ls,2)))\n",
    "    pair_df = pd.DataFrame(pair_ls,columns=['noun1','noun2'])\n",
    "\n",
    "    pair_df['noun1_freq'] = pair_df['noun1'].map(a_dict)\n",
    "    pair_df['noun2_freq'] = pair_df['noun2'].map(a_dict)\n",
    "\n",
    "    pair_df = shuffle(pair_df).reset_index(drop=True)\n",
    "\n",
    "    pair_df = balancedFreq_n1_n2(pair_df)\n",
    "\n",
    "    return pair_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_ls1 = df1_sample['noun']\n",
    "a_dict1 = {key:val for key,val in zip(df1_sample['noun'],df1_sample['freq'])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_ls = (list(combinations(noun_ls1,2)))\n",
    "pair_df = pd.DataFrame(pair_ls,columns=['n1','n2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n1</th>\n",
       "      <th>n2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>同谋</td>\n",
       "      <td>枯枝</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>同谋</td>\n",
       "      <td>前清</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>同谋</td>\n",
       "      <td>圆珠笔</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>同谋</td>\n",
       "      <td>头盖骨</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>同谋</td>\n",
       "      <td>分站赛</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   n1   n2\n",
       "0  同谋   枯枝\n",
       "1  同谋   前清\n",
       "2  同谋  圆珠笔\n",
       "3  同谋  头盖骨\n",
       "4  同谋  分站赛"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_ls1 = df1_sample['noun']\n",
    "pair_ls1 = (list(combinations(noun_ls1,2)))\n",
    "\n",
    "pair_df1 = pd.DataFrame(pair_ls1).rename(columns={0:'noun1',1:'noun2'})\n",
    "a_dict1 = {key:val for key,val in zip(df1_sample['noun'],df1_sample['freq'])}\n",
    "\n",
    "pair_df1['noun1_freq'] = pair_df1['noun1'].map(a_dict1)\n",
    "pair_df1['noun2_freq'] = pair_df1['noun2'].map(a_dict1)\n",
    "\n",
    "pair_df1['noun1_log'] = np.log(pair_df1['noun1_freq'])\n",
    "pair_df1['noun2_log'] = np.log(pair_df1['noun2_freq'])\n",
    "pair_df1 = shuffle(pair_df1).reset_index(drop=True)\n",
    "\n",
    "pair_df1 = balancedFreq_n1_n2(pair_df1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### clf_mod_noun structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_ls2 = df2_sample['noun']\n",
    "pair_ls2 = (list(combinations(noun_ls2,2)))\n",
    "\n",
    "pair_df2 = pd.DataFrame(pair_ls2).rename(columns={0:'noun1',1:'noun2'})\n",
    "a_dict2 = {key:val for key,val in zip(df2_sample['noun'],df2_sample['freq'])}\n",
    "\n",
    "pair_df2['noun1_freq'] = pair_df2['noun1'].map(a_dict2)\n",
    "pair_df2['noun2_freq'] = pair_df2['noun2'].map(a_dict2)\n",
    "\n",
    "pair_df2['noun1_log'] = np.log(pair_df2['noun1_freq'])\n",
    "pair_df2['noun2_log'] = np.log(pair_df2['noun2_freq'])\n",
    "pair_df2 = shuffle(pair_df2).reset_index(drop=True)\n",
    "\n",
    "pair_df2 = balancedFreq_n1_n2(pair_df2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### similarities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "/Users/yameiwang/miniconda3/envs/EMNLP2023_classifier/lib/python3.8/site-packages/scipy/spatial/distance.py:622: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "pair_df1 = similarity(pair_df1,noun_ls1)\n",
    "pair_df2 = similarity(pair_df2,noun_ls2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PMI\n",
    "##### clf_noun structure"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### clf_mod_structure"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### class membership"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_df1 = class_mem_calculator(pair_df1,df,noun_ls1)\n",
    "pair_df2 = class_mem_calculator(pair_df2,df,noun_ls2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EMNLP2023_classifier",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
